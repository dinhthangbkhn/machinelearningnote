{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast RCNN\n",
    "\n",
    "## 1. Some metric that you need to know:\n",
    "### 1.1 Precision \n",
    "measures **how accurate is your predictions**. i.e. the percentage of your predictions are correct.\n",
    "### 1.2 Recall\n",
    "measures **how good you find all the positives**. For example, we can find 80% of the possible positive cases in our top K predictions.\n",
    "### 1.3 Intersection over union (IoU) \n",
    "IoU measures the overlap between 2 boundaries. We use that to measure how much our predicted boundary overlaps with the ground truth (the real object boundary).\n",
    "\n",
    "<img src=\"IoU.png\">\n",
    "\n",
    "Model object detections are determined to be true or false depending upon the IoU threshold. This IoU threshold(s) for each competition vary, but in the COCO challenge, for example, 10 different IoU thresholds are considered, from 0.5 to 0.95 in steps of 0.05. For a specific object (say, ‘person’) this is what the precision-recall curves may look like when calculated at the different IoU thresholds of the COCO challenge.\n",
    "\n",
    "<img src=\"precision_recall_over_iou.png\">\n",
    "\n",
    "### 1.4 Average Precision (AP):\n",
    "To calculate the AP, for a specific class (say a “person”) the precision-recall curve is computed from the model’s detection output, by varying the model score threshold that determines what is counted as a model-predicted positive detection of the class.  \n",
    "\n",
    "<img src=\"Precision_Recall.jpeg\">\n",
    "\n",
    "The final step to calculating the AP score is to take the average value of the precision across all recall values as described in the graph above.\n",
    "The AP score is defined as the mean precision at the set of 11 equally spaced recall values, Recall_i = [0, 0.1, 0.2, …, 1.0]. Thus,  \n",
    "$$AP = \\frac{1}{11} \\sum_{Recall_{i}}Precision(Recall_{i}) $$\n",
    "The precision at recall i is taken to be the maximum precision measured at a recall exceeding $Recall_{i}$.\n",
    "\n",
    "\n",
    "### 1.5 mean Average Precision (mAP)\n",
    "Now that we’ve defined Average Precision (AP) and seen how the IoU threshold affects it, **the mean Average Precision or mAP score is calculated by taking the mean AP over all classes and/or over all IoU thresholds**, depending on the competition. For example: \n",
    "* PASCAL VOC2007 challenge only 1 IoU threshold was considered: 0.5 so the mAP was averaged over all 20 object classes.\n",
    "* For the COCO 2017 challenge, the mAP was averaged over all 80 object categories and all 10 IoU thresholds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fast RCNN architecture and training \n",
    "### 2.1 Introduce:\n",
    "The approach is similar to the R-CNN algorithm. But, instead of feeding the region proposals to the CNN, we <mark>feed the input image to the CNN</mark> to generate a convolutional feature map. From the convolutional feature map, we <mark>identify the region of proposals and warp them into squares</mark> and by <mark>using a RoI pooling layer we reshape them into a fixed size so that it can be fed into a fully connected layer</mark>. From the RoI feature vector, we use a <mark>softmax layer to predict the class of the proposed region and also the offset values for the bounding box</mark>.\n",
    "<img src=\"fast_rcnn.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The pooling layer\n",
    "The ROI pooling layers uses max pooling to convert the features inside any valid region of interest (RoI) into a small feature map with a fixed size *HxW*.  \n",
    "An RoI is a rectangular window into a conv feature map. Each RoI is defined by four tuple *(r,c,h,w)* that specifies its top-left corner *(r,c)* and its height and width *(h,w)*  \n",
    "RoI max pooling devides *hxw* RoI into an *HxW* grid of sub-windows of approximate size *h/H x w/W*, then max-pooling the values in each sub-window into the corresponding output grid cell.\n",
    "\n",
    "### 2.3 Multi-task loss \n",
    "A Fast RCNN network has two sibling output layers.   \n",
    "The first outputs a discrete probability distribution (per RoI), $p = (p_{0}, ..., p_{k})$ over $K+1$ categories.  \n",
    "The second outputs bouding-box regression offset, $t^{k}=(t^{k}_{x}, t^{k}_{y}, t^{k}_{w}, t^{k}_{h})$ for each of the $K$ object classes indexed by $k$ \n",
    "Each training RoI is labeled with a ground-truth class $u$ and a ground-truth bounding-box regression target $v$.\n",
    "We use a multi-task loss $L$ on each labeled RoI to jointly train for classification and bounding-box regression \n",
    "$$L(p,u,t^{u},v) = L_{cls}(p,u) + \\lambda[u > 1]L_{loc}(t^{u},v)$$\n",
    "in which $L_{cls}(p,u)=-log(p_{u})$ is log loss for true class $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bounding-box regression, we use the loss:\n",
    "$$L_{loc}(t^{u},v)=\\sum_{i {x,y,w,h}}smooth_{L_{1}}(t^{u}_{i}-v_{i}) $$\n",
    "in which   \n",
    "$$smooth_{L_{1}}(x)= 0.5x^{2}, if |x| < 1$$\n",
    "$$smooth_{L_{1}}(x)= |x|-0.5, otherwise$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Summary:\n",
    "The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 References\n",
    "* https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e\n",
    "* https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a\n",
    "* https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\n",
    "* https://arxiv.org/abs/1504.08083 (Fast RCNN paper )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
