{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to purchase a new car, will you walk up to the first car shop and purchase one based on the advice of the dealer? It’s highly unlikely.\n",
    "\n",
    "You would likely browser a few web portals where people have posted their reviews and compare different car models, checking for their features and prices. You will also probably ask your friends and colleagues for their opinion. In short, you wouldn’t directly reach a conclusion, but will instead make a decision considering the opinions of other people as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Voting Classifiers](#example)\n",
    "2. [Bagging](#example2)[TODO]\n",
    "3. [Random Forest](#third-example)[TODO]\n",
    "4. [Boosting](#fourth-examplehttpwwwfourthexamplecom)  \n",
    "4.1 [Ada Boost](#) [TODO]  \n",
    "4.2 [Gradient Boost](#) [TODO]  \n",
    "4.3 [XGBoost](#) [TODO]  \n",
    "4.4 [LightGBM](#) [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have trained a few classifiers, each one has 80% accuracy. You have a Logistic Regression classifier, a SVM classifier, a Random Forest classifier, and a few more. How do we create a better classifier based on them. \n",
    "\n",
    "\n",
    "A simple way to create it is to aggreate the prediction of each classifier and predict the class that gets the most votes. This majority vote classifier is called a **hard voting** classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.theconversation.com/files/193473/original/file-20171106-1041-b3hljk.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=926&fit=clip\" style=\"width:50%;margin-left: 200px;padding:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This voting classifier often achieves a higher accuracy than the best classifier in the emsemble. In fact, even if each classifier is a weak learner, the ensemble can still be a **strong learner**, provided there are a sufficient number of **weak learners** and they are still **sufficiently diverse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you will be confusing about what happenned. **How is this possible?** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be explained by a **\"law of large numbers\"**\n",
    "\n",
    "Suppose you have a slightly biased coin that has a 51% chance of coming up heads, and 49% chance of coming up tails. \n",
    "If you toss 1000 times, you will generally get more or less 510 heads and 490 tails. As you keep tossing the coin, \n",
    "the ratio of heads gets closer and closer to the probability of heads (51%)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Lawoflargenumbers.svg/400px-Lawoflargenumbers.svg.png\"\n",
    "     style=\"margin-left:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE 1**: ensemble methods work best when the predictors are as independent from one another as possible. \n",
    "One way to get diverse classifiers is to train them using very different algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "\n",
    "# fake some data \n",
    "y = np.array([1]*5000 + [0]*5000)\n",
    "X = np.random.normal(loc=1, scale=0.5, size=10000).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# train model\n",
    "log_model = LogisticRegression(solver='lbfgs')\n",
    "rnd_model = RandomForestClassifier(n_estimators=100)\n",
    "svc_model = SVC(gamma='scale')\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('lr', log_model), ('rf', rnd_model), ('svc', svc_model)],\n",
    "    voting='hard'\n",
    ")\n",
    "# voting_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.4893939393939394\n",
      "RandomForestClassifier 0.5051515151515151\n",
      "SVC 0.4990909090909091\n",
      "VotingClassifier 0.5006060606060606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for model in (log_model, rnd_model, svc_model, voting_model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, this result is not amazing because of my fake data. \n",
    "But I think you can learn how to do voting classifier after this example.\n",
    "**Let try it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE 2**: If all classifiers are able to estimate class probabilities, then you can tell sklearn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called **soft voting**.\n",
    "Now we will consider my example with *soft voting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.4893939393939394\n",
      "RandomForestClassifier 0.5060606060606061\n",
      "SVC 0.4990909090909091\n",
      "VotingClassifier 0.5054545454545455\n"
     ]
    }
   ],
   "source": [
    "# define model \n",
    "log_model = LogisticRegression(solver='lbfgs')\n",
    "rnd_model = RandomForestClassifier(n_estimators=100)\n",
    "svc_model = SVC(gamma='scale', probability=True)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('lr', log_model), ('rf', rnd_model), ('svc', svc_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# train and calculate accuracy \n",
    "for model in (log_model, rnd_model, svc_model, voting_model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
